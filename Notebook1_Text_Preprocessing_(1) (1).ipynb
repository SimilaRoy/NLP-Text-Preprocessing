{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0942866",
      "metadata": {
        "id": "c0942866"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. Text Preprocessing Importance in NLP\n",
        "2. Different Text Preprocessing Techniques\n",
        "3. Converting to Lower case\n",
        "4. Removal of HTML tags\n",
        "5. Removal of URLs\n",
        "6. Removing Numbers\n",
        "7. Converting numbers to words\n",
        "8. Apply spelling correction\n",
        "9. Convert accented characters to ASCII characters\n",
        "10. Converting chat conversion words to normal words\n",
        "11. Expanding Contractions\n",
        "12. Stemming\n",
        "13. Lemmatization\n",
        "14. Removal of Emojis\n",
        "15. Removal of Emoticons\n",
        "16. Converting Emojis to words\n",
        "17. Converting Emoticons to words\n",
        "18. Removing of Punctuations or Special Characters\n",
        "19. Removing of Stopwords\n",
        "20. Removing of Frequent words\n",
        "21. Removing of Rare words\n",
        "22. Removing single characters\n",
        "23. Removing Extra Whitespaces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abceca35",
      "metadata": {
        "id": "abceca35"
      },
      "source": [
        "### Text Preprocessing Importance in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb67df28",
      "metadata": {
        "id": "bb67df28"
      },
      "source": [
        "As we said before text preprocessing is the first step in the Natural Language Processing pipeline. The importance of preprocessing is increasing in NLP due to noise or unclear data extracted or collected from different sources.\n",
        "\n",
        "Most of the text data collected from reviews of E-commerce websites like Amazon or Flipkart, tweets from twitter,  comments from Facebook or Instagram, and other websites like Wikipedia, etc.\n",
        "\n",
        "We can observe users use short forms, emojis, misspelling of words, etc. in their comments, tweets, and so on.\n",
        "\n",
        "We should not feed raw data without preprocessing to  build models because the preprocessing of text directly improves the model's performance.\n",
        "\n",
        "If we feed data without performing any text preprocessing techniques, the build models will not learn the real significance of the data. In some cases, if we feed raw data without any preprocessing techniques the models will get confused and give random results.\n",
        "\n",
        "In that confusion, the model will learn harmful patterns that are not valuable. Due to this, the model's performance will be affected, which means the model performance will reduce significantly.\n",
        "\n",
        "So we should remove all these noises from the text and make it a more clear and structured form for building models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efaec33a",
      "metadata": {
        "id": "efaec33a"
      },
      "source": [
        "Here we have to know one thing.\n",
        "\n",
        "The natural language text preprocessing techniques will vary from problem to problem. This means we cannot apply the same text preprocessing techniques used for one NLP problem to another NLP problem.\n",
        "\n",
        "For example, in sentiment analysis classification problems, we can remove or ignore numbers within the text because numbers are not significant in this problem statement.\n",
        "\n",
        "However, we should not ignore the numbers if we are dealing with financial related problems. Because numbers play a key role in these kinds of problems.\n",
        "\n",
        "So while performing NLP text preprocessing techniques. We need to focus more on the domain we are applying these NLP techniques and the order of methods also plays a key role.\n",
        "\n",
        "Don't worry about the order of these techniques for now.  We will give the generic order in which you need to apply these techniques.\n",
        "\n",
        "Our suggestion is to use preprocessing methods or techniques on a subset of aggregate data (take a few sentences randomly). We can easily observe whether it is in our expected form or not. If it is in our expected form, then apply on a complete dataset; otherwise, change the order of preprocessing techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd18df2",
      "metadata": {
        "id": "fbd18df2"
      },
      "source": [
        "We will provide a python file with a preprocess class of all preprocessing techniques at the end of this article.\n",
        "\n",
        "You can download and import that class to your code. We can get preprocessed text by calling preprocess class with a list of sentences and sequences of preprocessing techniques we need to use.\n",
        "\n",
        "Again the order of technique we need to use will differ from problem to problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e554b1",
      "metadata": {
        "id": "a0e554b1"
      },
      "source": [
        "### Different Text Preprocessing Techniques\n",
        "Let us jump to learn different types of text preprocessing techniques.\n",
        "\n",
        "In the next few minutes, we will discuss and learn the importance and implementation of these techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9467e697",
      "metadata": {
        "id": "9467e697"
      },
      "source": [
        "#### Converting to Lower case\n",
        "Converting all our text into the lower case is a simple and most effective approach.  If we are not applying lower case conversion on words like NLP, nlp, Nlp, we are treating all these words as different words.\n",
        "\n",
        "After using the lower casing, all three words are treated as a single word that is nlp."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacf3f61",
      "metadata": {
        "id": "aacf3f61"
      },
      "source": [
        "\n",
        "### Implementation of lower case conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00cf07a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00cf07a0",
        "outputId": "2d06ecad-fbc4-4b50-9dda-aad407910951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example sentence for lower case conversion\n"
          ]
        }
      ],
      "source": [
        "def lower_case_convertion(text):\n",
        "\t\"\"\"\n",
        "\tInput :- string\n",
        "\tOutput :- lowercase string\n",
        "\t\"\"\"\n",
        "\tlower_text = text.lower()\n",
        "\treturn lower_text\n",
        "\n",
        "\n",
        "ex_lowercase = \"This is an example Sentence for LOWER case conversion\"\n",
        "lowercase_result = lower_case_convertion(ex_lowercase)\n",
        "print(lowercase_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf59b852",
      "metadata": {
        "id": "cf59b852"
      },
      "outputs": [],
      "source": [
        "## Output:: this is an example sentence for lower case conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0dd58e",
      "metadata": {
        "id": "2e0dd58e"
      },
      "source": [
        "### HTML Tag Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bccfaa12",
      "metadata": {
        "id": "bccfaa12"
      },
      "source": [
        "This is the second essential preprocessing technique. The chances to get HTML tags in our text data is quite common when we are extracting or scraping data from different websites.\n",
        "\n",
        "We don't get any valuable information from these HTML tags. So it is better to remove them from our text data. We can remove these tags by using regex and we can also use the BeautifulSoup module from bs4 libraries.\n",
        "\n",
        "Let us see the implementation using python.\n",
        "\n",
        "HTML tags removal Implementation using regex module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c387e8",
      "metadata": {
        "id": "c6c387e8"
      },
      "source": [
        "### HTML tags removal Implementation using regex module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc8ac0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dc8ac0e",
        "outputId": "acb73fd1-c0c3-488a-968a-35ac88f5e444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result :- \n",
            "   \n",
            " \n",
            " Hi, this is an example text with Html tags.  \n",
            " \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import re\n",
        "def remove_html_tags(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String without Html tags\n",
        "\tinput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\thtml_pattern = r'<.*?>'\n",
        "\twithout_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
        "\treturn without_html\n",
        "\n",
        "ex_htmltags = \"\"\" <body>\n",
        "<div>\n",
        "<h1>Hi, this is an example text with Html tags. </h1>\n",
        "</div>\n",
        "</body>\n",
        "\"\"\"\n",
        "htmltags_result = remove_html_tags(ex_htmltags)\n",
        "print(f\"Result :- \\n {htmltags_result}\")\n",
        "\n",
        "## Output:: Hi, this is an example text with Html tags."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0dd923f",
      "metadata": {
        "id": "b0dd923f"
      },
      "source": [
        "### Implementation of Removing HTML tags using bs4 library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ad4fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ad4fc5",
        "outputId": "6d91db1e-cb5e-4229-a533-48e8c10d57e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result :- \n",
            "   \n",
            " \n",
            " Hi, this is an example text with Html tags.  \n",
            " \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Implementation of Removing HTML tags using bs4 library\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "def remove_html_tags_beautifulsoup(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String without Html tags\n",
        "\tinput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\tparser = BeautifulSoup(text, \"html.parser\")\n",
        "\twithout_html = parser.get_text(separator = \" \")\n",
        "\treturn without_html\n",
        "\n",
        "ex_htmltags = \"\"\" <body>\n",
        "<div>\n",
        "<h1>Hi, this is an example text with Html tags. </h1>\n",
        "</div>\n",
        "</body>\n",
        "\"\"\"\n",
        "htmltags_result = remove_html_tags_beautifulsoup(ex_htmltags)\n",
        "print(f\"Result :- \\n {htmltags_result}\")\n",
        "\n",
        "## Output:: Hi, this is an example text with Html tags."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bbaa93",
      "metadata": {
        "id": "57bbaa93"
      },
      "source": [
        "### Removal of URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617ad1db",
      "metadata": {
        "id": "617ad1db"
      },
      "source": [
        "URL is the short-form of Uniform Resource Locator. The URLs within the text refer to the location of another website or anything else.\n",
        "\n",
        "If we are performing any website backlinks analysis, twitter or Facebook in that case, URLs are an excellent choice to keep in text.\n",
        "\n",
        "Otherwise, from URLs also we can not get any information. So we can remove it from our text. We can remove URLs from the text by using the python Regex library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7014f70",
      "metadata": {
        "id": "f7014f70"
      },
      "source": [
        "#### Implementation of Removing URLs  using python regex\n",
        "\n",
        "In the below script. We take example text with URLs and then call the 2 functions with that example text. In the remove_urls function, assign a regular expression to remove URLs to url_pattern after That, substitute URLs within the text with space by calling the re library's sub-function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da138fb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da138fb5",
        "outputId": "5d8625ad-926d-41dd-8994-b9c6117b969c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after removing URLs from text :- \n",
            " \n",
            "This is an example text for URLs like   &   etc.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Implementation of Removing URLs  using python regex\n",
        "\n",
        "import re\n",
        "def remove_urls(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String without URLs\n",
        "\tinput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\turl_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "\twithout_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
        "\treturn without_urls\n",
        "\n",
        "# example text which contain URLs in it\n",
        "ex_urls = \"\"\"\n",
        "This is an example text for URLs like http://google.com & https://www.facebook.com/ etc.\n",
        "\"\"\"\n",
        "\n",
        "# calling removing_urls function with example text (ex_urls)\n",
        "urls_result = remove_urls(ex_urls)\n",
        "print(f\"Result after removing URLs from text :- \\n {urls_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09432380",
      "metadata": {
        "id": "09432380"
      },
      "source": [
        "### Removing Numbers\n",
        "We can remove numbers from the text if our problem statement doesn't require numbers.\n",
        "\n",
        "For example, if we are working on financial related problems like banking or insurance-related sectors. We may get information from numbers.\n",
        "\n",
        "In those cases, we shouldn't remove numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a10999",
      "metadata": {
        "id": "e2a10999"
      },
      "source": [
        "#### Implementation of Removing numbers  using python regex\n",
        "In the code below, we will call the remove_numbers function with example text, which contains numbers.\n",
        "\n",
        "Let's see how to implement it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c629da69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c629da69",
        "outputId": "e6e8c53a-e780-434a-a56d-617f5a0d8c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after removing number from text :- \n",
            " \n",
            "This is an example sentence for removing numbers like  ,  , ,   ,  etc.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Implementation of Removing numbers  using python regex\n",
        "\n",
        "import re\n",
        "def remove_numbers(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String without numbers\n",
        "\tinput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\tnumber_pattern = r'\\d+'\n",
        "\twithout_number = re.sub(pattern=number_pattern,\n",
        " repl=\" \", string=text)\n",
        "\treturn without_number\n",
        "\n",
        "# example text which contain numbers in it\n",
        "ex_numbers = \"\"\"\n",
        "This is an example sentence for removing numbers like 1, 5,7, 4 ,77 etc.\n",
        "\"\"\"\n",
        "# calling remove_numbers function with example text (ex_numbers)\n",
        "numbers_result = remove_numbers(ex_numbers)\n",
        "print(f\"Result after removing number from text :- \\n {numbers_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cadec39b",
      "metadata": {
        "id": "cadec39b"
      },
      "source": [
        "In the above removing_numbers function. We mentioned a pattern to recognize numbers within the text and then substitute numbers with space using the re library's sub-function.\n",
        "\n",
        "And then return text after removing the number to numbers_result variable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52690217",
      "metadata": {
        "id": "52690217"
      },
      "source": [
        "### Converting numbers to words\n",
        "If our problem statement need valuable information from numbers in that case, we have to convert numbers to words. Similar problem statements which are discussed at the removing numbers (above section)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a185c9d",
      "metadata": {
        "id": "3a185c9d"
      },
      "source": [
        "### Implementation of Converting numbers to words using python num2words library\n",
        "We can convert numbers to words by just importing the num2words library. In the code below, we will call the num_to_words function with example text. Example text has numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60cbdc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60cbdc9",
        "outputId": "2c446258-0253-43af-d364-ca1f1215deb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a4e06e7babc96cef8f2ec8f001b2dd07d168396aa7fd192d556a96e987962fc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.13\n"
          ]
        }
      ],
      "source": [
        "#Install the package\n",
        "!pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d36f906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d36f906",
        "outputId": "a5fdf25d-1652-43f0-e72d-162780c5f169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after converting numbers to its words from text :- \n",
            " This is an example sentence for converting numbers to words like one to one, five to five, seventy-four to seventy-four, etc.\n"
          ]
        }
      ],
      "source": [
        "# function to convert numbers to words\n",
        "\n",
        "from num2words import num2words\n",
        "\n",
        "def num_to_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- text which have all numbers or integers in the form of words\n",
        "\tInput :- string\n",
        "\tOutput :- string\n",
        "\t\"\"\"\n",
        "\t# splitting text into words with space\n",
        "\tafter_spliting = text.split()\n",
        "\n",
        "\tfor index in range(len(after_spliting)):\n",
        "\t\tif after_spliting[index].isdigit():\n",
        "\t\t\tafter_spliting[index] = num2words(after_spliting[index])\n",
        "\n",
        "    # joining list into string with space\n",
        "\tnumbers_to_words = ' '.join(after_spliting)\n",
        "\treturn numbers_to_words\n",
        "\n",
        "# example text which contain numbers in it\n",
        "ex_numbers = \"\"\"\n",
        "This is an example sentence for converting numbers to words like 1 to one, 5 to five, 74 to seventy-four, etc.\n",
        "\"\"\"\n",
        "# calling remove_numbers function with example text (ex_numbers)\n",
        "numners_result = num_to_words(ex_numbers)\n",
        "print(f\"Result after converting numbers to its words from text :- \\n {numners_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4060a217",
      "metadata": {
        "id": "4060a217"
      },
      "source": [
        "In the above code, the num_to_words function is getting the text as input. In that, we are splitting text using a python string function of a split with space to get words individually.  \n",
        "\n",
        "Taking each word and checking if that word is digit or not. If the word is digit then convert that into words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f628c3b6",
      "metadata": {
        "id": "f628c3b6"
      },
      "source": [
        "### Apply spelling correction\n",
        "\n",
        "Spelling correction is another important preprocessing technique while working with tweets, comments, etc. Because we can see incorrect spelling words in those areas of text. We need to make those misspelling words to correct spelling words.\n",
        "\n",
        "We can check and replace misspelling words with correct spelling by using two python libraries, one is pyspellchecker, and another one is autocorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fb23b5",
      "metadata": {
        "id": "d4fb23b5"
      },
      "source": [
        "#### Implementation of spelling correction using python pyspellchecker library\n",
        "Below we are calling a spell_correction function with example text. Example text has incorrect spelling words to check whether the spell_correction function gives correct words or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb68ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edb68ba7",
        "outputId": "dc2e64a2-690b-4e61-9918-665a820cf185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ],
      "source": [
        "#Install pysepllchecker package\n",
        "\n",
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727e00a3",
      "metadata": {
        "id": "727e00a3"
      },
      "outputs": [],
      "source": [
        "# Implementation of spelling correction using python pyspellchecker library\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell_corrector = SpellChecker()\n",
        "\n",
        "# spelling correction using spellchecker\n",
        "def spell_correction(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- text which have correct spelling words\n",
        "\tInput :- string\n",
        "\tOutput :- string\n",
        "\t\"\"\"\n",
        "\t# initialize empty list to save correct spell words\n",
        "\tcorrect_words = []\n",
        "\t# extract spelling incorrect words by using unknown function of spellchecker\n",
        "\tmisSpelled_words = spell_corrector.unknown(text.split())\n",
        "\n",
        "\tfor each_word in text.split():\n",
        "\t\tif each_word in misSpelled_words:\n",
        "\t\t\tright_word = spell_corrector.correction(each_word)\n",
        "\t\t\tcorrect_words.append(right_word)\n",
        "\t\telse:\n",
        "\t\t\tcorrect_words.append(each_word)\n",
        "\n",
        "\t# joining correct_words list into single string\n",
        "\tcorrect_spelling = ' '.join(correct_words)\n",
        "\treturn correct_spelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e9919e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5e9919e",
        "outputId": "5d975389-8a07-4b7f-d7cb-3956d95fb4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after spell checking :- \n",
            "This is an example sentence for spell correction\n"
          ]
        }
      ],
      "source": [
        "#example text with mis spelling words\n",
        "ex_misSpell_words = \"\"\"\n",
        "This is an example sentence for spell corecton\n",
        "\"\"\"\n",
        "spell_result = spell_correction(ex_misSpell_words)\n",
        "print(f\"Result after spell checking :- \\n{spell_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2083670a",
      "metadata": {
        "id": "2083670a"
      },
      "source": [
        "### Implementation of spelling correction using python autocorrect library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcca779f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcca779f",
        "outputId": "225fcb8e-08e8-45b3-8b2a-cdc0d8b4bef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=6c46dc74246ff646b72ba6a8c64980b24293ebb48e0db96ed532db4686bd4281\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install autocorrect package\n",
        "!pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991532b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "991532b5",
        "outputId": "84f6146f-8b21-406f-c2da-165a7ac62430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result :- \n",
            "This is another example for spell correction\n"
          ]
        }
      ],
      "source": [
        "# Implementation of spelling correction using python autocorrect library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from autocorrect import Speller\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# spelling correction using spellchecker\n",
        "def spell_autocorrect(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- text which have correct spelling words\n",
        "\tInput :- string\n",
        "\tOutput :- string\n",
        "\t\"\"\"\n",
        "\tcorrect_spell_words = []\n",
        "\n",
        "\t# initialize Speller object for english language with 'en'\n",
        "\tspell_corrector = Speller(lang='en')\n",
        "\tfor word in word_tokenize(text):\n",
        "\t\t# correct spell word\n",
        "\t\tcorrect_word = spell_corrector(word)\n",
        "\t\tcorrect_spell_words.append(correct_word)\n",
        "\n",
        "\tcorrect_spelling = ' '.join(correct_spell_words)\n",
        "\treturn correct_spelling\n",
        "\n",
        "# another example text with misSpelling words\n",
        "ex_misSpell_words_1 = \"\"\"\n",
        "This is anoter exapl for spell correction\n",
        "\"\"\"\n",
        "spell_result = spell_autocorrect(ex_misSpell_words_1)\n",
        "print(f\"Result :- \\n{spell_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e08722",
      "metadata": {
        "id": "68e08722"
      },
      "source": [
        "### Convert accented characters to ASCII characters\n",
        "This is another common preprocessing technique in NLP. We can observe special characters at the top of the common letter or characters if we press a longtime while typing, for example, résumé.\n",
        "\n",
        "If we are not removing these types of noise from the text, then the model will consider resume and résumé; both are two different words.\n",
        "\n",
        "Even if both are the same. We can convert this accented character to ASCII characters by using the unidecode library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34073554",
      "metadata": {
        "id": "34073554"
      },
      "source": [
        "#### Implementation of accented text to ASCII converter in python\n",
        "We will define the accented_to_ascii function to convert accented characters to their ASCII values in the below script.  \n",
        "\n",
        "We will do this function with example text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a838890",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a838890",
        "outputId": "847fc278-f455-4ce2-9cf2-fabf2e95c31e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "# Install the unidecode package\n",
        "\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904c1c91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "904c1c91",
        "outputId": "f3849513-7d77-409e-bed6-e5456959af2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after converting accented characters to their ASCII values \n",
            "\n",
            "This is an example text with accented characters like deep learning and computer vision etc.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Implementation of accented text to ASCII converter in python\n",
        "\n",
        "import unidecode\n",
        "\n",
        "def accented_to_ascii(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- text after converting accented characters\n",
        "\tInput :- string\n",
        "\tOutput :- string\n",
        "\t\"\"\"\n",
        "\t# apply unidecode function on text to convert\n",
        "\t# accented characters to ASCII values\n",
        "\ttext = unidecode.unidecode(text)\n",
        "\treturn text\n",
        "\n",
        "# example text with accented characters\n",
        "ex_accented = \"\"\"\n",
        "This is an example text with accented characters like dèèp lèarning ánd cömputer vísíön etc.\n",
        "\"\"\"\n",
        "accented_result = accented_to_ascii(ex_accented)\n",
        "print(f\"Result after converting accented characters to their ASCII values \\n{accented_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44ec07d",
      "metadata": {
        "id": "e44ec07d"
      },
      "source": [
        "In the above code, we use the unidecode method of the unidecode library with input text. Which converts accented characters to ASCII values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446b278c",
      "metadata": {
        "id": "446b278c"
      },
      "source": [
        "### Converting chat conversion words to normal words\n",
        "This is another essential preprocessing technique if we work with chat conversions, or our problem statement requires chat conversion analysis. We need to handle short-form. As nowadays, people use short-form words in their chatting conversions for their simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007705af",
      "metadata": {
        "id": "007705af"
      },
      "source": [
        "A better way to work with those words is to replace short-form words to their original words.\n",
        "\n",
        "We can find all those short-form words and its actual words in this Github Repo to save that file into our system; click right click and then press on save as option.\n",
        "\n",
        "\n",
        "https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5fc79733",
      "metadata": {
        "id": "5fc79733"
      },
      "outputs": [],
      "source": [
        "\n",
        "# example text for chat conversation short-form words\n",
        "ex_chat = \"\"\"\n",
        "omg this is an example text for chat conversation.\n",
        "\"\"\"\n",
        "# open short_form file and then read sentences from text file using read())\n",
        "short_form_list = open('slang.txt', 'r')\n",
        "chat_words_str = short_form_list.read()\n",
        "\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "\tif line != \"\":\n",
        "\t\tcw = line.split(\"=\")[0]\n",
        "\t\tcw_expanded = line.split(\"=\")[1]\n",
        "\t\tchat_words_list.append(cw)\n",
        "\t\tchat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "fullform = line.split('=')[1] +' '\n",
        "\n",
        "ex =  line.split('=')[0]\n",
        "\n",
        "fullformchat = ex_chat.replace(ex,fullform)\n",
        "\n",
        "# calling function\n",
        "#chat_result = short_to_original(ex_chat)\n",
        "print(f\"Result {fullformchat}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a1c8c1",
      "metadata": {
        "id": "05a1c8c1"
      },
      "source": [
        " ### expanding contractions\n",
        "\n",
        "Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\n",
        "\n",
        "An example of a contraction word.\n",
        "\n",
        "\"don't\" is \"do not\"\n",
        "\"should've\" is \"should have\"\n",
        "Nlp models don't know about these contractions; they will consider \"don't\" and \"do not\" both are two different words.\n",
        "\n",
        "We have to choose this technique if our problem statement is required. Otherwise,  leave it as it is."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd34b80b",
      "metadata": {
        "id": "cd34b80b"
      },
      "source": [
        "#### Implementation of expanding contractions\n",
        "In the code below, we are importing the CONTRACTION_MAP dictionary from the contraction file. And then define expand_contractions function to expand contractions if our input text has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0e3d60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0e3d60",
        "outputId": "fc6537e8-1f12-420a-f515-9382a37629cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "#Install the package\n",
        "\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef5185b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ef5185b",
        "outputId": "1749531b-f852-4ad1-8e6b-28f6e59a8aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: I'll be there within 5 min. Shouldn't you be there too? \n",
            "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
            "          We've been waiting for this day for so long.\n",
            "Expanded_text: I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"
          ]
        }
      ],
      "source": [
        "# import library\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
        "          We've been waiting for this day for so long.'''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = []\n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))\n",
        "\n",
        "expanded_text = ' '.join(expanded_words)\n",
        "print('Original text: ' + text)\n",
        "print('Expanded_text: ' + expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4949bc76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4949bc76",
        "outputId": "4b2aecce-38d6-446e-af87-3b900f9515e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd0b41a",
      "metadata": {
        "id": "0cd0b41a"
      },
      "source": [
        "In the expand_contractions function, we take contraction words from our text matching with contraction map words. If we are not performing a lower case conversion technique before this, we have to take the first character to display the result of contraction \"Doesn't\" like \"Does not\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318b2607",
      "metadata": {
        "id": "318b2607"
      },
      "source": [
        "### Stemming\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6903f0",
      "metadata": {
        "id": "9c6903f0"
      },
      "source": [
        "Stemming is reducing words to their base or root form by removing a few suffix characters from words. Stemming is the text normalization technique.\n",
        "\n",
        "There are so many stemming algorithms available, but the most widely used one is porter stemming.\n",
        "\n",
        "For example, the result of books after stemming is a book, and the result of learning is learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6836e11e",
      "metadata": {
        "id": "6836e11e"
      },
      "source": [
        "But stemming doesn't always provide the correct form of words because this follows the rules like removing suffix characters to get base words.\n",
        "\n",
        "Sometimes, stemming words don't relate to original ones and sometimes give non - dictionary words or not proper words.  \n",
        "\n",
        "For this, we can observe in the above table results of stemming \"caring\" and \"console/consoling\". Because of these results stemming technique does not apply to all NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c966632a",
      "metadata": {
        "id": "c966632a"
      },
      "source": [
        "#### Implementation of Stemming using PorterStemming from nltk library\n",
        "In the below python script, we will define the porter_stemmer function to implement the stemming technique. We will call the function with example text.\n",
        "\n",
        "Before reaching the function, we have to initialize the object for the PorterStemmer class to use the stem function from that class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d008f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d008f7",
        "outputId": "5acf929b-db48-45e4-d69a-1ad134065119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after stemming technique :- \n",
            "program program with program languag\n"
          ]
        }
      ],
      "source": [
        "# Implementation of Stemming using PorterStemming from nltk library\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def porter_stemmer(text):\n",
        "\t\"\"\"\n",
        "\tResult :- string after stemming\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\t# word tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\n",
        "\tfor index in range(len(tokens)):\n",
        "\t\t# stem word to each word\n",
        "\t\tstem_word = stemmer.stem(tokens[index])\n",
        "\t\t# update tokens list with stem word\n",
        "\t\ttokens[index] = stem_word\n",
        "\n",
        "\t# join list with space separator as string\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# initialize porter stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "# example text for stemming technique\n",
        "ex_stem = \"Programers program with programing languages\"\n",
        "stem_result = porter_stemmer(ex_stem)\n",
        "print(f\"Result after stemming technique :- \\n{stem_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccaace51",
      "metadata": {
        "id": "ccaace51"
      },
      "source": [
        "In the porter_stemmer function, we tokenized the input using word_tokenize from the nltk library. And then, apply the stem function to each of the tokenized words and update the text with stemmer words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9836be3",
      "metadata": {
        "id": "d9836be3"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb46326",
      "metadata": {
        "id": "afb46326"
      },
      "source": [
        "The aim of usage of lemmatization is similar to the stemming technique to reduce inflection words to their original or base words. But the lemmatization process is different from the above approach.\n",
        "\n",
        "Lemmatization does not only trim the suffix characters; instead, use lexical knowledge bases to get original words. The result of lemmatization is always a meaningful word, not like stemming.\n",
        "\n",
        "The disadvantages of stemming people prefer to use lemmatization to get base or root words of original words. This preprocessing technique is also optional; we have to apply it based on our problem statement.\n",
        "\n",
        "Suppose we are doing POS (parts-of-speech) tagger problems. The original words of data have more information about data. As compared to stemming, the lemmatization speed is a little bit slow.\n",
        "\n",
        "Let's see the implementation of lemmatization using nltk library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "361ef785",
      "metadata": {
        "id": "361ef785"
      },
      "source": [
        "#### Implementation of lemmatization using nltk\n",
        "In the below strip, before calling the lemmatization function, we have to initialize the object for WordNetLemmatizer to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282f8396",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282f8396",
        "outputId": "eee5991a-a271-47e0-e023-3812131a28d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of lemmatization \n",
            "Programers program with programing language\n"
          ]
        }
      ],
      "source": [
        "## Implementation of lemmatization using nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatization(text):\n",
        "\t\"\"\"\n",
        "\tResult :- string after stemming\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\t# word tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\n",
        "\tfor index in range(len(tokens)):\n",
        "\t\t# lemma word\n",
        "\t\tlemma_word = lemma.lemmatize(tokens[index])\n",
        "\t\ttokens[index] = lemma_word\n",
        "\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# initialize lemmatizer object\n",
        "lemma = WordNetLemmatizer()\n",
        "# example text for lemmatization\n",
        "ex_lemma = \"\"\"\n",
        "Programers program with programing languages\n",
        "\"\"\"\n",
        "lemma_result = lemmatization(ex_lemma)\n",
        "print(f\"Result of lemmatization \\n{lemma_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfefac53",
      "metadata": {
        "id": "dfefac53"
      },
      "source": [
        "### Removal of Emojis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382b5f4b",
      "metadata": {
        "id": "382b5f4b"
      },
      "source": [
        "In today's online communication, emojis play a very crucial role.\n",
        "\n",
        "Emojis are small images. Users use these emojis to express their present feelings. We can communicate these with anyone globally. For some problem statements, we need to remove emojis from the text.\n",
        "\n",
        "Let's see on that type of problem statement how we can remove emojis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8703e739",
      "metadata": {
        "id": "8703e739"
      },
      "outputs": [],
      "source": [
        "# Implementation of emoji removing\n",
        "\n",
        "def remove_emojis(text):\n",
        "\t\"\"\"\n",
        "\tResult :- string without any emojis in it\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\temoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\twithout_emoji = emoji_pattern.sub(r'',text)\n",
        "\treturn without_emoji\n",
        "\n",
        "\n",
        "# example text for emoji removing technique\n",
        "ex_emoji = \"\"\"\n",
        "This is a test 😻 👍🏿\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2971ed3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2971ed3",
        "outputId": "504a5c80-b9a7-4696-d2cc-cab1ed73cc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result text after removing emojis :- \n",
            "\n",
            "This is a test  \n"
          ]
        }
      ],
      "source": [
        "# calling function\n",
        "emoji_result = remove_emojis(ex_emoji)\n",
        "print(f\"Result text after removing emojis :- \\n{emoji_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291bccb3",
      "metadata": {
        "id": "291bccb3"
      },
      "source": [
        "### Removal of emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72cc37ce",
      "metadata": {
        "id": "72cc37ce"
      },
      "source": [
        "Emojis and emoticons are both different. An emoticon portrays a human facial expression using just keyboard characters, such as letters, numbers, and punctuation marks.\n",
        "\n",
        "This is also the same as emojis; if problem statements don't require emoticons, we can remove them.\n",
        "\n",
        "Implementation of removing of emoticons\n",
        "To remove emotions from the text, we need a list of emoticons; in this GitHub Repo, we can find all emoticons as a dictionary.\n",
        "\n",
        "We take an EMOTICONS dictionary from that GitHub repo and save it in our system as emoticons_list.py. After that, import that file into our preprocessing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3724fd4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3724fd4f",
        "outputId": "2d6b8116-46bf-4db6-95c0-9193fe741e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ]
        }
      ],
      "source": [
        "# Install emot package\n",
        "\n",
        "!pip install emot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dc60b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11dc60b1",
        "outputId": "8a932524-5ffd-4535-8a06-03890bfcfa29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'value': ['☮', '🙂', '❤'],\n",
              " 'location': [[14, 15], [16, 17], [18, 19]],\n",
              " 'mean': [':peace_symbol:', ':slightly_smiling_face:', ':red_heart:'],\n",
              " 'flag': True}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import emot\n",
        "emot_obj = emot.core.emot()\n",
        "text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\"\n",
        "emot_obj.emoji(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a00f5e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a00f5e7",
        "outputId": "098f5ea1-912b-4566-8545-c3012b5bc583"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'value': ['☮', '🙂', '❤'],\n",
              "  'location': [[14, 15], [16, 17], [18, 19]],\n",
              "  'mean': [':peace_symbol:', ':slightly_smiling_face:', ':red_heart:'],\n",
              "  'flag': True},\n",
              " {'value': ['🙂', '❤'],\n",
              "  'location': [[14, 15], [16, 17]],\n",
              "  'mean': [':slightly_smiling_face:', ':red_heart:'],\n",
              "  'flag': True},\n",
              " {'value': ['☮', '❤'],\n",
              "  'location': [[14, 15], [16, 17]],\n",
              "  'mean': [':peace_symbol:', ':red_heart:'],\n",
              "  'flag': True},\n",
              " {'value': ['☮', '🙂'],\n",
              "  'location': [[14, 15], [16, 17]],\n",
              "  'mean': [':peace_symbol:', ':slightly_smiling_face:'],\n",
              "  'flag': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import emot\n",
        "emot_obj = emot.core.emot()\n",
        "bulk_test = [\"I love python ☮ 🙂 ❤ :-) :-( :-)))\", \"I love python 🙂 ❤ :-) :-( :-)))\", \"I love python ☮ ❤ :-) :-( :-)))\", \"I love python ☮ 🙂 :-( :-)))\"]\n",
        "\n",
        "emot_obj.bulk_emoji(bulk_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123e3964",
      "metadata": {
        "id": "123e3964"
      },
      "source": [
        "### Removing of Punctuations or Special Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a306cf8",
      "metadata": {
        "id": "1a306cf8"
      },
      "source": [
        "Punctuations or special characters are all characters except digits and alphabets. List of all available special characters are [!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~].  \n",
        "\n",
        "This is better to remove or convert emoticons before removing punctuations or special characters.\n",
        "\n",
        "If we apply this technique process before emoticons related techniques, we may lose emoticons from the text. So if we apply the emoticons technique, apply before removing the punctuation technique.\n",
        "\n",
        "For example, if we remove the period using the punctuation removing technique from text like \"money 20.98\", we will lose the period (.) between 20 & 98. That completely lost their meaning.\n",
        "\n",
        "So we have to focus more on choosing punctuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7b0bd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e7b0bd0",
        "outputId": "308a02d3-760d-4a87-8385-afbfde0d8fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after removing punctuations :- \n",
            "\n",
            "this is an example text for punctuations like \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Implementation of removing punctuations using string library\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String after removing punctuations\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\treturn text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "\n",
        "# example text for removing punctuations\n",
        "ex_punct = \"\"\"\n",
        "this is an example text for punctuations like .?/*\n",
        "\"\"\"\n",
        "punct_result = remove_punctuation(ex_punct)\n",
        "print(f\"Result after removing punctuations :- \\n{punct_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fe31eb",
      "metadata": {
        "id": "c0fe31eb"
      },
      "source": [
        "### Removing of Stopwords\n",
        "Stopwords are common words and irrelevant words from which we can't get any useful information for our model or problem statement.\n",
        "\n",
        "Few stopwords are \"a\", \"an\", \"the\", etc.  \n",
        "\n",
        "For example, we can ignore stop words when we work with sentiment analysis, text classification problems. But in the case of POS (Parts-Of-Speech) tagging or language translation, we have to consider whether stop words also give more information and useful words for our problem statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856adb7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "856adb7c",
        "outputId": "c7b1265d-7a16-4afb-d552-c0fca0a76855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing important package\n",
        "\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8d4f4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8d4f4a",
        "outputId": "426e326d-32f9-4324-9f32-37403bdb9511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e1b065",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1e1b065",
        "outputId": "aceb270b-b9af-4b1a-9a61-cf2fdcb5e4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Stopwords :- 412\n",
            "Result after removing stopwords :- \n",
            "example text stopwords , , .\n"
          ]
        }
      ],
      "source": [
        "# Implementation of removing stopwords using all stop words from nltk, spacy, gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import gensim\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- String after removing stopwords\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\ttext_without_sw = []\n",
        "\t# tokenization\n",
        "\ttext_tokens = word_tokenize(text)\n",
        "\tfor word in text_tokens:\n",
        "\t\t# checking word is stopword or not\n",
        "\t\tif word not in all_stopwords:\n",
        "\t\t\ttext_without_sw.append(word)\n",
        "\n",
        "\t# joining all tokens after removing stop words\n",
        "\twithout_sw = ' '.join(text_without_sw)\n",
        "\treturn without_sw\n",
        "\n",
        "\n",
        "# list of stopwords from nltk\n",
        "stopwords_nltk = list(stopwords.words('english'))\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "# list of stopwords from spacy\n",
        "stopwords_spacy = list(sp.Defaults.stop_words)\n",
        "# list of stopwords from gensim\n",
        "stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n",
        "\n",
        "# unique stopwords from all stopwords\n",
        "all_stopwords = []\n",
        "all_stopwords.extend(stopwords_nltk)\n",
        "all_stopwords.extend(stopwords_spacy)\n",
        "all_stopwords.extend(stopwords_gensim)\n",
        "# all unique stop words\n",
        "all_stopwords = list(set(all_stopwords))\n",
        "print(f\"Total number of Stopwords :- {len(all_stopwords)}\")\n",
        "\n",
        "# example text for stop words removing\n",
        "ex_sw = \"\"\"\n",
        "this is an example text for stopwords such as a, an, the etc.\n",
        "\"\"\"\n",
        "sw_result = remove_stopwords(ex_sw)\n",
        "\n",
        "print(f\"Result after removing stopwords :- \\n{sw_result}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e3af3b",
      "metadata": {
        "id": "20e3af3b"
      },
      "source": [
        "The code mentioned above, we take stopwords from different libraries such as nltk, spacy, and gensim.\n",
        "\n",
        "And then take unique stop words from all three stop word lists. In the remove_stopwords, we check whether the tokenized word is in stop words or not; if not in stop words list, then append to the text without the stopwords list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef73dc4d",
      "metadata": {
        "id": "ef73dc4d"
      },
      "source": [
        "In the above script, we defined two functions one is for counting frequent words another is to remove them from our corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c40a44c",
      "metadata": {
        "id": "6c40a44c"
      },
      "source": [
        "### Removing of Rare words\n",
        "Removing rare words text preprocessing technique is similar to eliminating frequent words. We can remove more irregular words from the corpus.\n",
        "\n",
        "#### Implementation of frequent words removing\n",
        "In the below script, the same as the above one, we defined two functions: finding rare words and removing them. We take only ten rare words for this sample text; this number may increase based on our text corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e1b1aba2",
      "metadata": {
        "id": "e1b1aba2",
        "outputId": "73809eba-f6e0-49f2-f687-732e139fc9f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is in\n",
            "raj likes\n"
          ]
        }
      ],
      "source": [
        "# Python program for the above approach\n",
        "from collections import Counter\n",
        "\n",
        "# Function to remove common\n",
        "# words from two strings\n",
        "def removeCommonWords(sent1, sent2):\n",
        "\n",
        "\t# Store the words present\n",
        "\t# in both the sentences\n",
        "\tsentence1 = list(sent1.split())\n",
        "\tsentence2 = list(sent2.split())\n",
        "\n",
        "\t# Calculate frequency of words\n",
        "\t# using Counter() function\n",
        "\tfrequency1 = Counter(sentence1)\n",
        "\tfrequency2 = Counter(sentence2)\n",
        "\n",
        "\n",
        "\tword = 0\n",
        "\n",
        "\t# Iterate the list consisting\n",
        "\t# of words in the first sentence\n",
        "\tfor i in range(len(sentence1)):\n",
        "\n",
        "\t\t# If word is present\n",
        "\t\t# in both the strings\n",
        "\t\tif sentence1[word] in frequency2.keys():\n",
        "\n",
        "\t\t\t# Remove the word\n",
        "\t\t\tsentence1.pop(word)\n",
        "\n",
        "\t\t\t# Decrease the frequency of the word\n",
        "\t\t\tword = word-1\n",
        "\t\tword += 1\n",
        "\n",
        "\tword = 0\n",
        "\n",
        "\t# Iterate the list consisting of\n",
        "\t# words in the second sentence\n",
        "\tfor i in range(len(sentence2)):\n",
        "\n",
        "\t\t# If word is present\n",
        "\t\t# in both the strings\n",
        "\t\tif sentence2[word] in frequency1.keys():\n",
        "\n",
        "\t\t\t# Remove the word\n",
        "\t\t\tsentence2.pop(word)\n",
        "\n",
        "\t\t\t# Decrease the removed word\n",
        "\t\t\tword = word-1\n",
        "\n",
        "\t\tword += 1\n",
        "\n",
        "\t# Print the remaining\n",
        "\t# words in the two sentences\n",
        "\tprint(*sentence1)\n",
        "\tprint(*sentence2)\n",
        "\n",
        "\n",
        "# Driver Code\n",
        "\n",
        "sentence1 = \"sky is blue in color\"\n",
        "sentence2 = \"raj likes sky blue color\"\n",
        "\n",
        "removeCommonWords(sentence1, sentence2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512a23ef",
      "metadata": {
        "id": "512a23ef"
      },
      "source": [
        "### Removing single characters\n",
        "After performing all text preprocessing techniques except extra spaces, removing this is better to remove a single character if there is any present in our corpus. We can remove using regex.\n",
        "\n",
        "Implementation of removing single characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06e441e",
      "metadata": {
        "id": "f06e441e",
        "outputId": "585cc11b-b726-45ef-a4d5-b44cfe9dcf47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result :-\n",
            "\n",
            "this is an example of single characters like , , and .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Remove single characters\n",
        "\n",
        "def remove_single_char(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- string after removing single characters\n",
        "\tInput :- string\n",
        "\tOutput:- string\n",
        "\t\"\"\"\n",
        "\tsingle_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
        "\twithout_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
        "\treturn without_sc\n",
        "\n",
        "# example text for removing single characters\n",
        "ex_sc = \"\"\"\n",
        "this is an example of single characters like a , b , and c .\n",
        "\"\"\"\n",
        "# calling remove_sc function to remove single characters\n",
        "sc_result = remove_single_char(ex_sc)\n",
        "print(f\"Result :-\\n{sc_result}\")\n",
        "\n",
        "## Output:: this is an example of single characters like , , and ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab89bee",
      "metadata": {
        "id": "dab89bee"
      },
      "source": [
        "### Removing Extra Whitespaces\n",
        "This is the last preprocessing technique. We can not get any information from extra spaces, so that we can ignore all additional spaces such as 0ne or more newlines, tabs, extra spaces.\n",
        "\n",
        "Our suggestion is to apply this preprocessing technique at last after performing all text preprocessing techniques.\n",
        "\n",
        "Implementation  of removing extra whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85329e9d",
      "metadata": {
        "id": "85329e9d",
        "outputId": "3b12cfbb-06f8-457c-8a31-69e4df498382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result :- \n",
            " this is an extra spaces . \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Removing Extra Whitespaces\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- string after removing extra whitespaces\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\tspace_pattern = r'\\s+'\n",
        "\twithout_space = re.sub(pattern=space_pattern, repl=\" \", string=text)\n",
        "\treturn without_space\n",
        "\n",
        "\n",
        "# example text for removing extra spaces\n",
        "ex_space = \"\"\"\n",
        "this      is an\n",
        "\n",
        "\n",
        "extra spaces        .\n",
        "\"\"\"\n",
        "\n",
        "space_result = remove_extra_spaces(ex_space)\n",
        "print(f\"Result :- \\n{space_result}\")\n",
        "\n",
        "## Output:: this is an extra spaces ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2162ed3",
      "metadata": {
        "id": "d2162ed3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f249358c",
      "metadata": {
        "id": "f249358c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341b7caf",
      "metadata": {
        "id": "341b7caf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75394628",
      "metadata": {
        "id": "75394628"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed5a51ed",
      "metadata": {
        "id": "ed5a51ed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}